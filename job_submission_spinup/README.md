# Job Scheduling Tutorial

## Introduction 

This tutorial will introduce you to the concept of "Jobs" in High Performance Computing (HPC), why they are necessary, explore different types of jobs, and provide examples for you to work through. Note that this tutorial is meant to be an extension of the Super Computing Spin Up: part 2 Job Submissions course.

:warning: **__This tutorial assumes you are using CU Research Computing resources, the following jobs and scripts will ONLY work on the CURC system. Visit [our docs](https://curc.readthedocs.io/en/latest/access/logging-in.html) if you need an account and login directions__** :warning:

The basic directory structure of this tutorial follows:

```
 ResearchComputing/Supercomputing_Spinup_Spring_2022/job_submission/ 
	README.md 		# *this* tutorial markdown file
	/scripts		# directory which includes skeleton scripts for tutorial examples
	/programs		# directory which includes programs our jobs will execute 
	/solutions 		# directory with solution job scripts for examples
	/output			# directory where example job output is directed
```

We'll start with a brief introduction to jobs and why they are necessary, describe the main job types, and work through some examples. All of the examples in this section will require access to a terminal. We'll finish up with some non-traditional job submissions which allow for access to RC hardware via graphical browser application.

## HPC Jobs

High Performance Computing (HPC) systems are more often than not, environments where many users share/compete for the same resources. Because our HPC clusters are shared resources with many users trying to utilize available compute with their applications, we need a system to divide compute in a simple and fair system.

CU Research Computing uses the resource management program SLURM (Simple Linux Utility for Resource Management) to manage resources. Through SLURM, users can request allotments of computer resources called "Jobs." Jobs are scheduled to a queue where they will wait until resources become available and then run.

Through SLURM, users can request jobs in 2 way:

1) Batch jobs (most common)

2) Interactive jobs

Both types of jobs give a user access to a compute node (hardware where most all computation should be run, visit [our docs](https://curc.readthedocs.io/en/latest/compute/node-types.html) for description of different node types at CURC). 

### Batch Jobs

The primary method of running applications on Research Computing (and other HPC centers) resources is through a batch job. A batch job is a job that runs on a compute node with little or no interaction with the users

Unlike running an application on your personal machine, you do not call the application you wish to run directly. Instead you create a job script that includes a call to your application. Job scripts are simply a set of resource requests and commands. When a job script is run, all the commands in the job script are executed on a compute node.

__The primary advantage of a batch job is that you don't have to sit around waiting for resources to become available, just schedule a job and wait for your results.__

### Interactive Jobs

As the name would imply, an interactive job is a job that allows users to interact with requested resources in real time. Users can run applications, execute scripts, or run other commands directly on a compute node.

__The primary advantage of an interactive job is that you get real-time access to a compute node. Do note that you will have to wait for resources to become available which can often take a long time.__ 


## Example 1: Schedule your first batch job 

Most of our examples are batch jobs as these are the most common and often trickiest to get familiar with.

We'll start off with simply running a pre-written script to get familiar with the syntax. You run your job script by passing it to the Slurm queue with the `sbatch` command followed by your job script name:
```
sbatch <your-jobscript-name>
```

From this directory run the command:
```
sbatch scripts/submit_test.sh
```
:warning: Reminder, this example is meant to run on CU Research Computing's system :warning:

Once you schedule this job you should see a message in standard output (i.e. your terminal screen):
```
Submitted batch job <jobid>
```
This lets you know that your job has been successfully scheduled. Congrats! You just scheduled your first job.

But what did this job actually do? Where did the output go?

### Job Output

Once a job completes its execution, the standard output of the script will be redirected to an output file. This is super useful for debugging and understanding what your job is actually doing. Note that this output could be different from output generated by your application.

The output file is created in directory job was run unless specified in your script by a specific output directive (like a command in your job script to tell the Slurm scheduler how to run the job). Don't worry about directives for now, we'll get there in just a bit.

If the directive --output is not provided then a generic file name will be used (slurm_xxxxxx.out). In this tutorial all example scripts have their output sent to the `/output` directory to keep things cleaned up.

> *_Note: all jobs should be run from this directory (where the README.md is located) to ensure output goes to the correct `/output` directory._*

### Check output of your first job:

You can check the output of this first job with the `cat` command which will print the contents of a file to standard output.
```
$ cat output/test_<jobid>.out
This is a test of user <your_userid>
```

### Check the test job script:

Great! You have successfully scheduled your first job and checked the output! Now let's check how the heck the job script actually did this. You can open up the test script using your preferred text editor (`vim` or `nano` on RC systems).

Open up the test script and try to see if you can understand what's going on:
```
$ vim scripts/submit_test.sh 	# you can use nano instead of vim for a more intuitive editor
```
You should see something like this:
```
#!/bin/bash

## Directives
#SBATCH --ntasks=1                      # Number of requested cores 
#SBATCH --time=0:01:00                  # Max wall time
#SBATCH --partition=atesting            # Specify amilan testing compute node
#SBATCH --output=./output/test_%j.out   # Rename standard output file

## Software
module purge                        # Purge all existing modules

## User commands
echo "This is a test of user $USER" 

```
You should recognize some of the things we did even if you don't understand all of the syntax.

- We has asked for the output to be put into the `/output` directory, and renamed it. You can use the `%j` variable to insert the job ID (super useful to distinguish output files).

- We saw the output which just printed the sentence `This is a test of user $USER` using the bash `echo` command. Here we use the `$USER` variable in place of a hard coded name to get *your* user name.

We'll get to some of the other syntax in the next section.

## The Batch Script 

Batch scripts can look pretty confusing (especially the larger ones) but the basic anatomy of a batch script follows only 3 parts: directives, software, and user scripting. I always start my scripts with this boilerplate code to make sure I'm not forgetting something.

```
 #!/bin/bash

 ## Directives (Resources to request) 
 #SBATCH --<resource>=<amount> 

 ## Software
 module load <software>

 ## User scripting
 <command>
```
- Directives are comments that are included at the top of a job script that tells the shell information about the script:
	- The first directive, the shebang directive, is always on the first line of any script. The directive indicates which shell you want running commands in your job. 
	- The next directives that must be included with your job script are sbatch directives. These directives specify resource requirements to Slurm for a batch job. An sbatch directive is written as such:
		- `#SBATCH --<resource>=<amount>`
	- There are a TON of different directives (more than we can cover here), but you can find the main ones used at RC in [our docs](https://curc.readthedocs.io/en/latest/running-jobs/batch-jobs.html#job-flags) or at the [SLURM website](https://slurm.schedmd.com/sbatch.html).
- Any shared software that is needed must be loaded via the job script. At RC shared software is managed through [modules](https://curc.readthedocs.io/en/latest/compute/modules.html). Software can be loaded in a job script just like it would be on the command line.
	- First we will purge all software that may be left behind from your working environment		with:
		- `module purge`
	- After this you can load whatever software you need by running the following command:
		- `module load <software>`
- The last part of a job script is the actual user scripting that will execute when the job is executing. This part of the job script includes all user commands that are needed to set up and execute the desired task. Any Linux command can be utilized in this step.

More details on these 3 sections can be found at our [batch script docs](https://curc.readthedocs.io/en/latest/running-jobs/batch-jobs.html#).

So taking a look back at `scripts/submit_test.sh` we can see the different directives that were used, specifically:
- `#SBATCH --ntasks=1` which requested 1 core
- `#SBATCH --time=0:01:00` which requested a max time of 1 minute
- `#SBATCH --partition=atesting` which requested a quick turn-around testing specific compute node
- `#SBATCH --output=test_%j.out` which requested an output file of the name `test_<jobid>.out` to be created in the `./output` directory

We didn't request any software (we just purged software just in case), and we ran the linux `echo` command. We'll take a look at some more complex examples next!

### A note about partitions and QOS:
Some of the more common directives you will see at RC (and other HPC institutions) are `partition` and `qos`.
- Partitions are simply types of nodes users can request (CPU, GPU, high memory, testing node). All RC partitions are cluster specific. You can find Summit partitions [here](https://curc.readthedocs.io/en/latest/running-jobs/job-resources.html#partitions-summit). We'll stick to testing partitions today as they give us quick turn-around for non intensive jobs.
- QOS stands for `Quality of Service` and is used to constrain or modify the characteristics that a job can have.  This could come in the form of specifying a QoS to request for a longer run time or a high priority queue for condo owned nodes. You can find Summit QOS's [here](https://curc.readthedocs.io/en/latest/running-jobs/job-resources.html#quality-of-service-summit).

## Example 2: Your turn!

This next example is going to involve you creating your own job script with the following specifications:
1) Create a script called `submit_sleep.sh` in the `/scripts` directory.
- You can create a new file with `nano` or `vim`
2) The job should have the following parameters:

- The job will run on 1 core of 1 node
- We will request a 1 minute wall time
- Run on the amilan testing partition
- Set the output file to be named “sleep.%j.out” in the `/output` directory
- Name your job “sleep”
- Bonus: Email yourself when the job ends

3) The job should contain the following user commands:
```
 echo "Running on host" `hostname`
 echo "Starting Sleep"
 sleep 30
 echo "Ending Sleep. Exiting Job!"
```

There are some directives here that we haven't seen yet, here is a quick cheatsheet:
```
Partition: --partition=<partition_name>
Sending emails: --mail-type=<type>
Email address: --mail-user=<user>
Number of nodes: --nodes=<nodes>
Number of cores: --ntasks=<number-of-cores>
Wall time: --time=<wall time>
Job Name: --job-name=<jobname>
Output: --output=<output_name>
```
Things to keep in mind: 
- Your output will default to whatever directory you run your script from, so keep in mind where `/output` is in relation to where you want to run it from.
- Is any software needed? If not, what should we have in the software section? 

Once you feel like you have everything done, go ahead and schedule it and see if you get the results you'd like. You can always check in the `/solutions` directory if you get stuck. 

## Checking your jobs

You may notice that the sleep job took a while to complete. This is the way most HPC jobs work as well; they are not instantaneous and will take time. We may want to check on our jobs while they are running. There are many different ways to do so, and we'll cover a handful here.

- `squeue`: Monitor your jobs status in queue and while running:
	- By Default shows all jobs in queue you can narrow this down with:
	```
	$ squeue –u <username>
	$ squeue –p <partition>
	```
- `sacct`: Check back on usage statistics of previous Jobs
	- By default only checks all your jobs from the start of the current day, narrow this down with:
	``` 
	$ sacct –u <username>
	$ sacct --start=MM/DD/YY –u <username>
	$ sacct –j <job-id>
	```
- `scontrol`: Advanced command usually used by system administrators, but you can use it too!
	```
	$ scontrol show job <job number>
	```
- `seff`: Utility to check efficiency post-job
	- First load in the `slurmtools` software module, then check job efficiency:
	```
	$ module load slurmtools
	$ seff <job number>
	```

## Software

So running a simple job is not too bad... but what if I want to run a job with specific software?

LMOD is the module management system we use at RC. Most software is not accessible by default and must be loaded in. This allows Research Computing to provide multiple versions of the software concurrently and enables users to easily switch between different versions. Modules will modify environment variables to make your desired software visible to your terminal.

You can only see modules from a compile or compute node (read more about node types [here](https://curc.readthedocs.io/en/latest/compute/node-types.html)). So to see available software without going to a compute node, first login to a compile node with `ssh scompile` 

From there you can use the `module avail` command to see a list of compiler independent software. If you need compiler dependent software, you will need to first load in a compiler and/or MPI (read more about compiler dependent software and compiling software [here](https://curc.readthedocs.io/en/latest/compute/compiling.html)).

You can load in software using the `module load <software>` command.

> **What if my software isn't available through LMOD?** 
> - Software must be installed locally if not available through LMOD. Users are encouraged to compile their own software locally.
> - RC User Support is happy to assist, but note that installs are best effort. Email us for software policy and install support.

Some examples of LMOD commands for `matlab`:
```
$ module purge           #Unloads all current modules
$ module unload matlab   #Unloads matlab
$ module spider matlab   #Searches for matlab in module tree
```

## Example 3: Serial R code

Next, we're going to schedule a job that runs R on an R script. We're not going to worry too much about the R script itself (this is a job submission tutorial after all), but if you don't use R yourself think about how your own software would be run (we'll look at other examples later too).

This next job will run from a script that calls and runs the `R_program.R` script in the `/programs` directory.

- Taking a quick look at the R script with `vim programs/R_program.R` we see that it just creates a vector of the planets, then prints off that vector. Nothing too wild.

- Next, let's take a look at the job script with `vim scripts/submit_R.sh`. Note how R is loaded (in practice it's best to specify the version with something like `module load R/3.3.0` but that's not too important here).

Your job here is just to make sure the output is going to the right place (remember, where you schedule from matters), have the script change into the `/programs` directory (again note where you run it from), then run the R script. You can run an R script with `Rscript <script name>`.

If you get stuck, you can take a look at the solution at `solutions/answer_submit_R.sh`


## Example 4: Serial Matlab code

Next, we'll create a job script from scratch that run's a serial matlab program and schedule it.

The instructions for this script are as follows:
1) Name it `submit_matlab.sh` and put it in `/scripts` (or wherever you want, you now have the understanding to run jobs from wherever!).
2) Load the `matlab` module
3) Run the matlab script. The job should contain the following user command:
```
cd programs 
matlab –nodisplay –nodesktop –r "matlab_tic;"
# the "nodisplay" and "nodesktop" will keep matlab from trying to open up the GUI
```
4) The batch script should have the following parameters:
- The job will run on 1 core of 1 node
- We will request a 2 minute wall time
- Run on the amilan testing partition
- Set the output file to be named “matlab.%j.out”
- Name your job “matlab”
	- Bonus: Email yourself when the job ends

## Advanced Script, running an MPI Job:

To truly take advantage of the HPC environment (and if our software allows for it) we can parallelize our code -- or run it across multiple CPU cores. In this case, the number of cores (or `ntasks` will always be greater than 1). One of the common use cases is using software than can take advantage of the Message Passing Interface (or MPI). MPI is a library standard for handling parallel processing. MPI is also compatible with multi-node structures, allowing for very large, multi-node applications (i.e, distributed memory models). You can learn more about the fundamentals of parallel computing at our [docs](https://curc.readthedocs.io/en/latest/programming/parallel-programming-fundamentals.html). 

If you were using MPI compatible software, you could first load in a compiler (gcc or intel) then a version of MPI (openMPI or IMPI respectively) for the specified compiler version.

An example of what this might look like in practice is:
```
#!/bin/bash
#SBATCH --nodes=1			# Number of requested nodes
#SBATCH --ntasks=4			# Number of requested cores
#SBATCH --time=0:01:00			# Max walltime
#SBATCH --qos=normal	      		# Specify QOS
#SBATCH --amilan		        # Specify Summit haswell nodes
#SBATCH --output=python_%j.out		# Output file name

# purge all existing modules
module purge

# Load the python module
module load python/3.5.1
module load intel impi   

# Run Python Script
cd programs 
mpirun -np 4 python hello1.py
```

In this case we're using an MPI compatible python library called mpi4py which allows us to use write python code that can be run across multiple cores/nodes. You can check out the program at `/programs/hello1.py`. One of the commands that can be used to run MPI compatible programs is `mpirun`, you can find more about MPI best practices at RC [here](https://curc.readthedocs.io/en/latest/programming/MPIBestpractices.html).

### Parallelize Serial Code

Not all code is designed to run with MPI (nor always makes sense to do so). Suppose you have a very simple serial program that crops a photo, and you need to apply it to crop several million photos. You could rewrite the serial program into a parallel program that would utilize multiple processors to more quickly run the program over the entire set of photos (compared to doing one-at-a-time), but this would require some knowledge of parallel programming. Even worse, if your code is in a language that has limited parallelization capabilities, so this may not be an option. 

RC has a couple different tools that let's users run serial programs in parallel, the implementation is outside the scope of this tutorial, but all of the tools (requesting multiple cores & nodes) have been discussed here. You can find more about these tools at:
- [RC LoadBalancer](https://curc.readthedocs.io/en/latest/software/loadbalancer.html)
- [GNU Parallel](https://curc.readthedocs.io/en/latest/software/GNUParallel.html)

## Interactive Jobs

Sometimes we want our job to run in the background and sometimes we want to work on program in real time, maybe we want to test workflows or debug. Interactive jobs are a great way to do this. Interactive jobs allow a user to interact with applications in real time within an HPC environment. With interactive jobs, users request time and resources to work on a compute node directly. Users can then run graphical user interface (GUI) applications, execute scripts, or run other commands directly on a compute node.

You can request an interactive job by using the `sinteractive` command. Unlike the `sbatch`, resources must be requested via the command line through the use of flags. Though running sinteractive without any flags is possible, this will result in default values being used for your jobs. For example, you can provide a `qos` and a `time` parameter to avoid long queue times or accidental overuse of your priority.

```
sinteractive --qos=interactive --time=00:10:00
```
Go ahead and try this command from a `login` or `acompile` node. You will see the output:
```
/usr/local/bin/sinteractive: waiting for job with ID <jobid> to start.
```
Once the resources you requested become available, you will be logged into a compute node:
```
<user>@c3cpu-:~$
``` 
From here, you can load in any module's you'd like, and run programs straight from the command line! Pretty neat, huh? You can try opening up an interactive python prompt with:
```
<user>@c3cpu:~ $ module load anaconda
(base) <user>@c3cpu:~ $ python --version
Python 3.8.5
(base) <user>@c3cpu~ $ python
Python 3.8.5 (default, Sep  4 2020, 07:30:14) 
[GCC 7.3.0] :: Anaconda, Inc. on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> print("hello world!")
hello world!
>>> 
```
Here, I'm using `conda` to load in python. Conda is a great tool to manage and install python and R packages (and more!) which we highly reccomend using at RC. More info on `conda` at [out docs](https://curc.readthedocs.io/en/latest/software/python.html).

You can also open up GUI applications with interactive jobs. You have to enable X11 graphical forwarding which will vary based on OS. Instructions to do so can be found [here](https://curc.readthedocs.io/en/latest/running-jobs/interactive-jobs.html#interactive-gui-applications).

## Other examples:

There are a handful of other examples you can check out in the `/scripts` directory. We are still actively developing this series and will add more content and develop content over time. If you have specific thing's you'd like to see added (or encounter errors) please email us at rc-help@colorado.edu.

## Interactive Browser Applications

We'll finish off with some discussion on interactive browser applications. If you are new to linux, scheduling jobs on an HPC system can be pretty indimidating and overwhelming. Many researchers don't want to learn an entire extra field and just want to get to their research! We have several browser based applications that give you the power to connect to a compute node straight away. The entire process of scheduling a job is made easier (usually defaults are enough) and you can get onto your work.

Do note that the cost of using these methods will limit your resources available to you, though often if you are programming interactively you don't need as many resources. 

We currently have a couple of interactive applications that some users will recognize.
- [RC JupyterHub](https://curc.readthedocs.io/en/latest/gateways/jupyterhub.html) 

	:information_source: only CU and CSU users
- [EnginFrame Virtual Desktop](https://curc.readthedocs.io/en/latest/gateways/enginframe.html) 

	:information_source: only for CU users

These are being phased out for a new tool called [Open OnDemand](https://curc.readthedocs.io/en/latest/gateways/OnDemand.html) which will unify all of these previous tools (and more!) into a single browser application. With OnDemand you can access:

:information_source: Only for CU users at the moment

- JupyterHub
- A Virtual Desktop 
- Matlab
- Open terminals in the browser itself
- Navigate your files on the RC system
- and even view your current jobs and compose jobs

This is a tool we are super excited about and can't wait to use more going forward.


## End

If you have any questions please feel free to contact rc-help@colorado.edu
